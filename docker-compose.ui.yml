
services:
  # Lobe Chat - Primary operator UI
  lobe-chat:
    image: lobehub/lobe-chat:v1.40.5
    container_name: lobe-chat
    ports:
      - "3210:3210"
    environment:
      # OpenAI-compatible API configuration (pointing to LocalAI)
      - OPENAI_API_KEY=sk-dummy-key-for-localai
      - API_KEY_SELECT_MODE=manual
      - OPENAI_PROXY_URL=http://localai:8080/v1
      
      # Database configuration (optional, for persistence)
      - DATABASE_URL=postgresql://lobe:lobe@lobe-db:5432/lobe
      - DATABASE_DRIVER=node
      
      # S3 storage for file uploads (optional)
      # - S3_ENDPOINT=http://minio:9000
      # - S3_BUCKET=lobe-chat
      # - S3_ACCESS_KEY_ID=minioadmin
      # - S3_SECRET_ACCESS_KEY=minioadmin
      
      # Authentication (optional)
      # - NEXT_AUTH_SECRET=your-secret-here
      # - NEXTAUTH_URL=http://localhost:3210
      
      # Feature flags
      - FEATURE_FLAGS=enableWebRTC,enableImageGeneration
      
      # Telemetry
      - TELEMETRY_ENABLED=false
    volumes:
      - lobe-chat-data:/app/.next/cache
    depends_on:
      - lobe-db
    networks:
      - agentic-network
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "wget", "--quiet", "--tries=1", "--spider", "http://localhost:3210/api/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s

  # PostgreSQL database for Lobe Chat persistence
  lobe-db:
    image: postgres:17.2-alpine
    container_name: lobe-db
    environment:
      POSTGRES_USER: lobe
      POSTGRES_PASSWORD: lobe
      POSTGRES_DB: lobe
    volumes:
      - lobe-db-data:/var/lib/postgresql/data
    networks:
      - agentic-network
    healthcheck:
      test: ["CMD", "pg_isready", "-U", "lobe"]
      interval: 10s
      timeout: 5s
      retries: 5
    restart: unless-stopped

  # MinIO - S3-compatible storage for file uploads (optional)
  minio:
    image: minio/minio:RELEASE.2024-12-18T13-15-44Z
    container_name: lobe-minio
    command: server /data --console-address ":9001"
    ports:
      - "9000:9000"  # S3 API
      - "9001:9001"  # Web Console
    environment:
      MINIO_ROOT_USER: minioadmin
      MINIO_ROOT_PASSWORD: minioadmin
    volumes:
      - minio-data:/data
    networks:
      - agentic-network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9000/minio/health/live"]
      interval: 30s
      timeout: 10s
      retries: 3
    restart: unless-stopped

  # ---------------------------------------------------------------------------
  # Open-Lovable - UI/codegen tool
  # ---------------------------------------------------------------------------
  # https://github.com/firecrawl/open-lovable
  # BUILDKIT_STARTER_SPEC.md Layer 1.20: UI/codegen tool (gated by eval + PR checks)
  open-lovable:
    image: ghcr.io/nicepkg/gpt-runner:latest
    container_name: open-lovable
    ports:
      - "3211:3000"
    environment:
      NODE_ENV: production
      OPENAI_API_KEY: ${OPENAI_API_KEY:-sk-dummy-key-for-localai}
      OPENAI_BASE_URL: ${OPENAI_BASE_URL:-http://localai:8080/v1}
    networks:
      - agentic-network
    healthcheck:
      test: ["CMD", "wget", "-q", "--spider", "http://localhost:3000"]
      interval: 30s
      timeout: 10s
      retries: 3
    restart: unless-stopped

volumes:
  lobe-chat-data:
  lobe-db-data:
  minio-data:
  open-lovable-data:

networks:
  agentic-network:
    external: true
    name: agentic-network

# Usage:
# 1. Ensure network exists: docker network create agentic-network
# 2. Ensure LocalAI is running: docker-compose -f docker-compose.localai.yml up -d
# 3. Start UI services: docker-compose -f docker-compose.ui.yml up -d
# 4. Access Lobe Chat: http://localhost:3210
# 5. Access MinIO Console: http://localhost:9001 (minioadmin/minioadmin)
#
# Configuration:
# - Lobe Chat will use LocalAI as the backend LLM provider
# - All conversations are stored in PostgreSQL
# - File uploads are stored in MinIO (S3-compatible)
# - No external API keys required (fully local)
#
# First-time setup:
# 1. Open http://localhost:3210
# 2. Go to Settings â†’ Language Model
# 3. Select "Custom OpenAI" provider
# 4. Base URL: http://localai:8080/v1
# 5. API Key: sk-dummy-key-for-localai (any value works)
# 6. Select your model from the dropdown
