[workspace]
name = "robostack"
description = "Development environment for RoboStack ROS packages"
channels = ["robostack-humble", "conda-forge"]
platforms = ["linux-64", "osx-64", "osx-arm64", "linux-aarch64"]

[dependencies]
python = ">=3.11,<3.13"

# Build tools
compilers = ">=1.11.0,<2"
cmake = ">=3.28,<4"
pkg-config = ">=0.29.2,<0.30"
make = ">=4.4.1,<5"
ninja = ">=1.13.2,<2"

# Compilation cache & fast linking
ccache = ">=4.10,<5"
sccache = ">=0.8,<1"

# Note: Rust toolchain for Agent Gateway comes from Nix, not pixi
# (cargo/rust not available on all platforms in conda-forge)

# Archive/Network tools (for tree-sitter)
tar = ">=1.34"
curl = ">=8.0"

# Node.js ecosystem (for LazyVim plugins like peek.nvim, live-preview.nvim)
nodejs = ">=22.0,<23"    # LTS "Jod" - active until Apr 2027
pnpm = ">=9.0"

# ROS specific tools
rosdep = ">=0.26.0,<0.27"
colcon-common-extensions = ">=0.3.0,<0.4"
ros-humble-desktop = ">=0.10.0,<0.11"
catkin_tools = ">=0.8.2,<0.10"

# Testing & Code Quality
pytest = ">=8.0,<9"
pytest-cov = ">=4.0,<6"
mypy = ">=1.0,<2"
black = ">=24.0,<25"
isort = ">=5.0,<6"

# Scientific/ML dependencies (explicit)
numpy = ">=1.24,<3"
pandas = ">=2.0,<3"
scikit-learn = ">=1.3,<2"

# Jupyter & Interactive Development
jupyterlab = ">=4.3,<5"
ipython = ">=8.30,<9"
ipywidgets = ">=8.0,<9"
notebook = ">=7.0,<8"

# LLMOps & Evaluation
mlflow = ">=2.19,<3"
tensorboard = ">=2.0,<3"
wandb = ">=0.19,<1"

# Hugging Face ecosystem
transformers = ">=4.47,<5"
accelerate = ">=1.2,<2"
sentence-transformers = ">=3.3,<4"
datasets = ">=3.2,<4"
tokenizers = ">=0.21,<1"

# Messaging & Orchestration (BUILDKIT_STARTER_SPEC.md Layer 6)
nats-py = ">=2.9"           # NATS Python client for event bus
temporalio = ">=1.7"        # Temporal Python SDK for durable workflows

# =============================================================================
# Agent Runtime Packages (BUILDKIT_STARTER_SPEC.md Layer 7: P2-001 to P2-005)
# =============================================================================
# P2-001: agno-agi/agno - Multi-agent framework, runtime, and control plane
# Available on PyPI: https://pypi.org/project/agno/
agno = ">=0.1"              # Latest: 2.3.24, Requires Python >=3.7,<4

# P2-002: ruvnet/agentic-flow - CANNOT BE INSTALLED VIA PIXI
# This is a Node.js/TypeScript package, not a Python package
# Repository: https://github.com/ruvnet/agentic-flow
# Install via npm: npm install agentic-flow@alpha

# P2-003: ruvnet/claude-flow - CANNOT BE INSTALLED VIA PIXI
# This is a Node.js/TypeScript package, not a Python package
# Repository: https://github.com/ruvnet/claude-flow
# Install via npm: npm install -g claude-flow@alpha

[pypi-dependencies]
# P2-004: ruvnet/Synaptic-Mesh - CANNOT BE INSTALLED VIA PIP
# This is a Rust/TypeScript/WASM project, not a Python package
# Repository: https://github.com/ruvnet/Synaptic-Mesh
# Multi-language project with Rust core, TypeScript CLI, WebAssembly compilation
# Install via: npx synaptic-mesh init (after cloning and building)

# P2-005: ruvnet/daa - CANNOT BE INSTALLED VIA PIP
# This is a Rust SDK with Node.js bindings (NAPI), not a Python package
# Repository: https://github.com/ruvnet/daa
# Decentralized Autonomous Applications SDK written in Rust
# Use the Rust crates directly or Node.js bindings

# =============================================================================
# Additional PyPI-only dependencies
# =============================================================================

# Build tools (JavaScript/TypeScript)
esbuild = ">=0.24,<1"

# Data processing
datafusion = ">=44.0"
polars = ">=1.18"

# PyTorch ML stack (CPU by default, CUDA via feature)
# Using conda-forge for native performance and ROS2 compatibility
# See docs/CONFLICTS.md for Python 3.14 and CUDA analysis
# Note: Version coupling - PyTorch 2.5.x requires torchvision 0.20.x, torchaudio 2.5.x
pytorch = ">=2.5,<3"
torchvision = ">=0.20,<1"
torchaudio = ">=2.5,<3"

[feature.cuda]
# CUDA-enabled PyTorch (requires NVIDIA GPU)
# Activate with: pixi run -e cuda <command>
platforms = ["linux-64"]
channels = ["pytorch", "nvidia", "conda-forge"]

[feature.cuda.dependencies]
# PyTorch with CUDA from pytorch channel
# Version coupling: PyTorch 2.5.x requires torchvision 0.20.x, torchaudio 2.5.x
pytorch-cuda = { version = "12.4.*", channel = "pytorch" }
pytorch = { version = ">=2.5,<2.6", channel = "pytorch", build = "*cuda*" }
torchvision = { version = ">=0.20,<0.21", channel = "pytorch", build = "*cuda*" }
torchaudio = { version = ">=2.5,<2.6", channel = "pytorch", build = "*cuda*" }

# =============================================================================
# AIOS Agent OS Feature
# =============================================================================
# AIOS (AI Agent Operating System) requires strict dependency pinning
# to ensure compatibility with the agent kernel and Cerebrum SDK.
# See: https://github.com/agiresearch/AIOS
#      https://github.com/agiresearch/Cerebrum
# Activate with: pixi run -e aios <command>

[feature.aios]
# AIOS requires Python 3.10-3.11 (no 3.12+ support)
platforms = ["linux-64", "osx-64", "osx-arm64", "linux-aarch64"]

[feature.aios.dependencies]
# Core AIOS dependencies with strict pins for compatibility
pydantic = "==2.7.0"
numpy = "==1.24.3"

# LLM & Embedding dependencies
litellm = ">=1.0"
transformers = ">=4.30"
accelerate = ">=0.20"
sentence-transformers = ">=2.0"

# Vector database (AIOS default)
chromadb = ">=0.4"

# Cache & messaging
redis-py = ">=4.5.1"

# API server
fastapi = ">=0.100"
uvicorn = ">=0.20"

# Utilities
python-dotenv = ">=1.0"
watchdog = ">=2.1.9"
rich = ">=13.0"
click = "==8.1.7"

# NLP
nltk = ">=3.8"

# Cerebrum SDK dependencies
requests = ">=2.28"
platformdirs = ">=3.0"
datasets = ">=2.14"

# Cerebrum Agent SDK (P0 - Required for AIOS agent development)
# See: https://github.com/agiresearch/Cerebrum
pyautogen = ">=0.2"         # AutoGen for multi-agent orchestration

[feature.aios.pypi-dependencies]
# Cerebrum SDK - AIOS Agent Development Kit (not in conda-forge)
# P0: Required per BUILDKIT_STARTER_SPEC.md Layer 7
aios-agent-sdk = ">=0.1"

# =============================================================================
# AIOS + CUDA Feature
# =============================================================================
# AIOS with GPU acceleration via vLLM
# Activate with: pixi run -e aios-cuda <command>

[feature.aios-cuda]
platforms = ["linux-64"]

[feature.aios-cuda.dependencies]
# Includes all AIOS dependencies plus vLLM
pydantic = "==2.7.0"
numpy = "==1.24.3"
litellm = ">=1.0"
transformers = ">=4.30"
accelerate = ">=0.20"
sentence-transformers = ">=2.0"
chromadb = ">=0.4"
redis-py = ">=4.5.1"
fastapi = ">=0.100"
uvicorn = ">=0.20"
python-dotenv = ">=1.0"
watchdog = ">=2.1.9"
rich = ">=13.0"
click = "==8.1.7"
nltk = ">=3.8"
requests = ">=2.28"
platformdirs = ">=3.0"
datasets = ">=2.14"

# =============================================================================
# LLMOps Feature - Evaluation & Experiment Tracking
# =============================================================================
# TruLens for runtime LLM evaluation and MLflow for experiment tracking
# BUILDKIT_STARTER_SPEC.md Layer 12: LLMOps & Evaluation
# Activate with: pixi run -e llmops <command>

[feature.llmops]
platforms = ["linux-64", "osx-64", "osx-arm64", "linux-aarch64"]

[feature.llmops.dependencies]
# TruLens - Runtime LLM evaluation
# See: https://github.com/truera/trulens
trulens-eval = ">=0.30"

# MLflow - Experiment tracking & model registry
# See: https://github.com/mlflow/mlflow
mlflow = ">=2.10"

# OpenTelemetry for instrumentation
opentelemetry-api = ">=1.20"
opentelemetry-sdk = ">=1.20"
opentelemetry-exporter-otlp = ">=1.20"

# Evaluation dependencies
pandas = ">=2.0"
scipy = ">=1.10"
scikit-learn = ">=1.3"

# =============================================================================
# Finetuning Feature - LLM Training & Optimization (P2-010)
# =============================================================================
# Unsloth for fast LLM finetuning: 2x faster training with 80% less memory
# BUILDKIT_STARTER_SPEC.md Layer 12: LLMOps & Evaluation
# Activate with: pixi run -e finetuning <command>
#
# GPU REQUIREMENTS:
# - NVIDIA GPU with CUDA support (CUDA 11.8+ or 12.1+)
# - Minimum 8GB VRAM recommended (varies by model size)
# - For production: 16GB+ VRAM (e.g., RTX 4090, A100, H100)
# - CPU-only mode available but defeats the performance benefits
#
# CONDA-FORGE AVAILABILITY:
# - Unsloth is NOT available on conda-forge
# - Must be installed via PyPI (pip)
# - Dependencies (PyTorch, transformers, accelerate) are in base environment
#
# USAGE:
# - Combine with CUDA feature for GPU acceleration:
#   pixi run -e finetuning-cuda python train.py
# - Or use default (CPU-only, not recommended):
#   pixi run -e finetuning python train.py

[feature.finetuning]
platforms = ["linux-64", "osx-64", "osx-arm64", "linux-aarch64"]

[feature.finetuning.dependencies]
# Core dependencies for finetuning (already in base, but explicit for clarity)
pytorch = ">=2.5,<3"
transformers = ">=4.47,<5"
accelerate = ">=1.2,<2"
datasets = ">=3.2,<4"
tokenizers = ">=0.21,<1"

# Training utilities
tqdm = ">=4.66"
peft = ">=0.8"        # Parameter-Efficient Fine-Tuning
bitsandbytes = ">=0.44"  # 8-bit optimizers

[feature.finetuning.pypi-dependencies]
# Unsloth - Fast LLM finetuning (PyPI-only, not on conda-forge)
# See: https://github.com/unslothai/unsloth
unsloth = ">=2024.0"

# =============================================================================
# Caching Feature - LLM Prompt & Response Caching (P3-001, P3-002)
# =============================================================================
# Semantic caching solutions for AI/LLM workloads to reduce latency and costs
# BUILDKIT_STARTER_SPEC.md Layer 10: State & Storage
# Activate with: pixi run -e caching <command>
#
# P3-001: vCache - Semantic prompt caching with guaranteed error bounds
# P3-002: prompt-cache - LLM proxy for semantic caching (Go binary in flake.nix)

[feature.caching]
platforms = ["linux-64", "osx-64", "osx-arm64", "linux-aarch64"]

[feature.caching.dependencies]
# Core dependencies for caching operations
python = ">=3.11,<3.13"
redis-py = ">=4.5.1"         # Redis client for cache backend

# OpenAI for embedding and inference (vCache default)
openai = ">=1.0"

# Vector database support
numpy = ">=1.24,<3"

[feature.caching.pypi-dependencies]
# P3-001: vCache - Semantic Prompt Cache with Error Bounds
# See: https://github.com/vcache-project/vCache
# Description: First semantic prompt cache that guarantees user-defined error rate bounds
# Features:
#   - Reduces LLM latency by up to 100x and costs by up to 10x
#   - Online-learned, embedding-specific decision boundaries
#   - Supports FIFO, LRU, MRU, and custom SCU eviction policies
#   - Modular: configurable inference engines, embeddings, vector DBs
# Installation: pip install -e . (from cloned repo - not yet on PyPI)
# Usage:
#   from vcache import VCache, VerifiedDecisionPolicy
#   policy = VerifiedDecisionPolicy(delta=0.01)
#   vcache = VCache(policy=policy)
#   response = vcache.infer("Is the sky blue?")
# NOTE: Requires OpenAI API key via environment variable
# NOTE: Not yet available on PyPI - install from source:
#   git clone https://github.com/vcache-project/vCache.git
#   cd vCache && pip install -e .

[environments]
default = { features = [], solve-group = "default" }
cuda = { features = ["cuda"], solve-group = "cuda" }
aios = { features = ["aios"], solve-group = "aios" }
aios-cuda = { features = ["aios", "aios-cuda", "cuda"], solve-group = "aios-cuda" }
llmops = { features = ["llmops"], solve-group = "llmops" }
finetuning = { features = ["finetuning"], solve-group = "finetuning" }
finetuning-cuda = { features = ["finetuning", "cuda"], solve-group = "finetuning-cuda" }
caching = { features = ["caching"], solve-group = "caching" }
