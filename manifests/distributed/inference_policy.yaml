# Multi-Model Fan-Out (MOE) Policy Configuration
# BUILDKIT_STARTER_SPEC.md Section 10.4: Parallel Inference Execution
#
# This policy governs how change-making tasks are processed through
# multiple models in parallel for consensus-based decision making.
#
# Requirements per spec:
# - Local tier: >= 5 small local models (target <3B active params)
# - Cloud tier: >= 2 strong cloud coders/reasoners in parallel
# - Reducer: Deterministic merge + policy gate (OPA) + eval gate

inference_policy:
  name: flexstack-moe
  version: "1.0.0"
  description: "Multi-model ensemble for change-making tasks"

  # Trigger conditions for MOE inference
  trigger:
    task_types:
      - code_generation
      - code_modification
      - configuration_change
      - deployment_decision
      - security_review
    # Skip MOE for simple queries
    skip_for:
      - information_retrieval
      - documentation_lookup
      - status_check

  # Local tier configuration (>= 5 models required)
  local_tier:
    min_parallel: 5
    max_parallel: 8
    timeout_ms: 30000
    backend: localai  # LocalAI OpenAI-compatible API

    models:
      # Model 1: General purpose (Gemma)
      - name: gemma-3n-E2B-it
        file: gemma-3n-E2B-it-UD-Q4_K_XL.gguf
        source: https://huggingface.co/unsloth/gemma-3n-E2B-it-GGUF
        weight: 1.0
        specialization: general
        max_tokens: 4096
        context_length: 8192

      # Model 2: Reasoning specialist (Phi-4)
      - name: Phi-4-mini-reasoning
        file: Phi-4-mini-reasoning-UD-Q4_K_XL.gguf
        source: https://huggingface.co/unsloth/Phi-4-mini-reasoning-GGUF
        weight: 1.2
        specialization: reasoning
        max_tokens: 4096
        context_length: 16384

      # Model 3: Code specialist (DeepSeek R1)
      - name: DeepSeek-R1-Distill-Qwen-1.5B
        file: DeepSeek-R1-Distill-Qwen-1.5B-Q8_0.gguf
        source: https://huggingface.co/unsloth/DeepSeek-R1-Distill-Qwen-1.5B-GGUF
        weight: 1.1
        specialization: code
        max_tokens: 4096
        context_length: 32768

      # Model 4: General quantized (Gemma 4B)
      - name: gemma-3-4b-it-qat
        file: gemma-3-4b-it-qat-UD-Q4_K_XL.gguf
        source: https://huggingface.co/unsloth/gemma-3-4b-it-qat-GGUF
        weight: 0.9
        specialization: general
        max_tokens: 4096
        context_length: 8192

      # Model 5: Code/General (Qwen3 4B)
      - name: Qwen3-4B
        file: Qwen3-4B-Q4_K_M.gguf
        source: https://huggingface.co/unsloth/Qwen3-4B-GGUF
        weight: 1.0
        specialization: code
        max_tokens: 4096
        context_length: 32768

      # Optional: Lightweight models for simple tasks
      - name: gemma-3-1b-it
        file: gemma-3-1b-it-BF16.gguf
        source: https://huggingface.co/unsloth/gemma-3-1b-it-GGUF
        weight: 0.7
        specialization: general
        optional: true

      - name: Qwen3-0.6B
        file: Qwen3-0.6B-BF16.gguf
        source: https://huggingface.co/unsloth/Qwen3-0.6B-GGUF
        weight: 0.5
        specialization: general
        optional: true

  # Cloud tier configuration (>= 2 providers required)
  cloud_tier:
    min_parallel: 2
    max_parallel: 4
    timeout_ms: 60000
    fallback_on_timeout: true

    providers:
      # Primary: Claude Code CLI
      - name: claude-code
        type: cli
        command: claude
        weight: 1.5
        priority: 1
        capabilities:
          - code_generation
          - reasoning
          - analysis
        rate_limit:
          requests_per_minute: 20

      # Secondary: Codex CLI
      - name: codex
        type: cli
        command: codex
        weight: 1.3
        priority: 2
        capabilities:
          - code_generation
          - code_completion
        rate_limit:
          requests_per_minute: 30

      # Tertiary: GitHub Copilot
      - name: github-copilot
        type: cli
        command: gh copilot
        weight: 1.0
        priority: 3
        capabilities:
          - code_generation
          - code_explanation
        rate_limit:
          requests_per_minute: 30

      # OpenRouter fallback (API)
      - name: openrouter
        type: api
        base_url: https://openrouter.ai/api/v1
        weight: 0.8
        priority: 4
        capabilities:
          - code_generation
          - reasoning
        rate_limit:
          requests_per_minute: 60

  # Reducer configuration
  reducer:
    strategy: weighted_consensus
    min_agreement: 0.6  # 60% of weighted votes must agree

    # Policy gate (OPA integration)
    policy_gate:
      enabled: true
      endpoint: opa://inference/merge
      timeout_ms: 5000
      fail_action: reject  # reject, warn, or passthrough

    # Conflict resolution
    conflict_resolution:
      method: highest_weight_wins
      fallback: cloud_tier_primary  # If no consensus, use top cloud provider

    # Response merging
    merge_strategy:
      code_generation: syntax_diff_merge
      reasoning: longest_chain_of_thought
      configuration: conservative_merge

    # Quality thresholds
    quality_threshold:
      min_confidence: 0.7
      require_explanation: true
      max_response_diff: 0.3  # Max divergence between responses

  # Audit and logging
  audit:
    log_all_responses: true
    store_disagreements: true
    trace_id_propagation: true
    retention_days: 30

    # Export to observability stack
    export:
      prometheus_metrics: true
      loki_logs: true
      jaeger_traces: true

    # Metrics to track
    metrics:
      - latency_per_model
      - consensus_rate
      - disagreement_frequency
      - policy_gate_rejections
      - timeout_rate

  # Feature flags for A/B testing
  feature_flags:
    enable_local_tier: true
    enable_cloud_tier: true
    enable_policy_gate: true
    enable_quality_threshold: true
    experimental:
      chain_of_thought_voting: false
      semantic_similarity_merge: false
